{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWoxuclFCj26"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules import ModuleList\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm_notebook, trange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##helps in casting input tensor to dimensions nf.\n",
        "##used to cast input to query,key and value.\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nx, nf):\n",
        "        super().__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = nn.Parameter(w)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QV_aY_1SCpPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##normal neural network which adds gelu activation and dropout\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
        "        super().__init__()\n",
        "        self.c_fc = Conv1D(d_model, nx)\n",
        "        self.c_proj = Conv1D(nx, d_model)\n",
        "        self.act = F.gelu\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"
      ],
      "metadata": {
        "id": "0DkwY9gMC0wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##scaled dot-product attention with multiple heads\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "        self.c_attn = Conv1D(d_model, d_model*3)\n",
        "        self.scale = scale\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.c_proj = Conv1D(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"return shape [`batch`, `head`, `sequence`, `features`]\"\n",
        "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head)\n",
        "        x = x.view(*new_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def _attn(self, q, k, v, attn_mask=None):\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "        if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
        "        nd, ns = scores.size(-2), scores.size(-1)\n",
        "        if attn_mask is not None: scores = scores + attn_mask\n",
        "        scores = self.softmax(scores)\n",
        "        scores = self.dropout(scores)\n",
        "        outputs = torch.matmul(scores, v)\n",
        "        return outputs\n",
        "\n",
        "    def merge_heads(self, x):\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
        "        return x.view(*new_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_attn(x)\n",
        "        q, k, v = x.split(self.d_model, dim=2)\n",
        "        q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
        "        out = self._attn(q, k, v)\n",
        "        out = self.merge_heads(out)\n",
        "        out = self.c_proj(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "lAt6fZMCC4uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##single transformer block consisting of normalization layer,attention block and feedforward layer\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
        "        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n",
        "        self.ln_1 = LayerNorm(d_model)\n",
        "        self.ln_2 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.feedforward(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "yzU5I1KfC-y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##GPT-2 architecture consists of 12 transformer blocks.\n",
        "def _get_clones(module, n):\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(n)])\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.nlayers = nlayers\n",
        "        block = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n",
        "        self.h = _get_clones(block, 12)\n",
        "        self.wte = nn.Embedding(vcb_sz, d_model)\n",
        "        self.wpe = nn.Embedding(n_ctx, d_model)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.ln_f = LayerNorm(d_model)\n",
        "        self.out = nn.Linear(d_model, vcb_sz, bias=False)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.out.weight = self.wte.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, src, labels=None, pos_ids=None):\n",
        "        if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
        "        inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\n",
        "        for i in range(self.nlayers): inp = self.h[i](inp)\n",
        "        inp = self.ln_f(inp)\n",
        "        logits = self.out(inp)\n",
        "        outputs = (logits,) + (inp,)\n",
        "\n",
        "        if labels is not None:\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "            return outputs\n",
        "        return logits"
      ],
      "metadata": {
        "id": "TNtRhfSXDCI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##model evaluation using pretrained weights\n",
        "model = GPT2()\n",
        "\n",
        "model_dict = model.state_dict()\n",
        "state_dict = torch.load(\"./gpt2-pytorch_model.bin\") #pretrained weights\n",
        "\n",
        "old_keys = []\n",
        "new_keys = []\n",
        "for key in state_dict.keys():\n",
        "    if \"mlp\" in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights\n",
        "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
        "        new_keys.append(new_key)\n",
        "        old_keys.append(key)\n",
        "\n",
        "for old_key, new_key in zip(old_keys, new_keys):\n",
        "    state_dict[new_key]=state_dict.pop(old_key)\n",
        "\n",
        "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "\n",
        "model_dict.update(pretrained_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "model.eval() #model in inference mode as it's now initialized with pretrained weights"
      ],
      "metadata": {
        "id": "R7lAFvQ5DFxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Sample-text generation\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "context   = torch.tensor([tokenizer.encode(\"The planet earth\")])\n",
        "\n",
        "def generate(context, ntok=20):\n",
        "    for _ in range(ntok):\n",
        "        out = model(context)\n",
        "        logits = out[:, -1, :]\n",
        "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = np.NINF\n",
        "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
        "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
        "    return context\n",
        "\n",
        "out = generate(context, ntok=20)\n",
        "tokenizer.decode(out[0])"
      ],
      "metadata": {
        "id": "rMm0rJuYDYP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DlM1GoxKDgyA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}